<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RAG, Finetuning, and Prompt Engineering - Extending the Capabilities of LLMs | Danial Amin </title> <meta name="author" content="Danial Amin"> <meta name="description" content="Large Language Models have revolutionized AI with their ability to understand and generate human-like text. However, these models have inherent limitations in their knowledge and capabilities. This comprehensive guide explores three key techniques that have emerged to address these limitations and extend LLM capabilities."> <meta name="keywords" content="human-computer interaction, generative ai, personas, user representation, fairness"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?05bb9ffd5c923af1a6eccf0d57836de3"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://danial-amin.github.io/blog/2025/RAG-works/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.technique-diagram{text-align:center;font-family:monospace;background:#f8f9fa;padding:1.5rem;border:2px solid #dee2e6;border-radius:.5rem;margin:2rem 0}.key-insight{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:white;padding:1.5rem;border-radius:.5rem;margin:2rem 0}.comparison-highlight{background:#e8f5e8;border-left:4px solid #4caf50;padding:1rem;margin:1.5rem 0;border-radius:.25rem}.technique-box{background:#fff3e0;border:1px solid #ff9800;padding:1rem;border-radius:.25rem;margin:1rem 0}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "RAG, Finetuning, and Prompt Engineering - Extending the Capabilities of LLMs",
            "description": "Large Language Models have revolutionized AI with their ability to understand and generate human-like text. However, these models have inherent limitations in their knowledge and capabilities. This comprehensive guide explores three key techniques that have emerged to address these limitations and extend LLM capabilities.",
            "published": "March 26, 2025",
            "authors": [
              
              {
                "author": "Danial Amin",
                "authorURL": "https://linkedin.com/in/danial-amin",
                "affiliations": [
                  {
                    "name": "Samsung Design Innovation Center, France",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Danial</span> Amin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>RAG, Finetuning, and Prompt Engineering - Extending the Capabilities of LLMs</h1> <p>Large Language Models have revolutionized AI with their ability to understand and generate human-like text. However, these models have inherent limitations in their knowledge and capabilities. This comprehensive guide explores three key techniques that have emerged to address these limitations and extend LLM capabilities.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</a> </div> <div> <a href="#finetuning">Finetuning</a> </div> <div> <a href="#prompt-engineering">Prompt Engineering</a> </div> <div> <a href="#similarities-between-the-approaches">Similarities Between the Approaches</a> </div> <div> <a href="#key-differences">Key Differences</a> </div> <div> <a href="#choosing-the-right-approach">Choosing the Right Approach</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <p>Large Language Models (LLMs) have revolutionized artificial intelligence with their ability to understand and generate human-like text. However, these models have inherent limitations in their knowledge and capabilities. Three key techniques have emerged over the time to address these limitations and extend LLM capabilities: Retrieval-Augmented Generation (RAG), finetuning, and prompt engineering.</p> <div class="key-insight"> <strong>Core Challenge:</strong> While LLMs possess remarkable general capabilities, they face temporal, domain, and contextual boundaries that limit their effectiveness in specialized applications. The solution lies in strategic enhancement techniques that address these specific limitations. </div> <p>This comprehensive guide explores each approach, their purposes, and how they compare in extending LLM capabilities beyond their inherent constraints.</p> <h2 id="retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</h2> <p>RAG enhances LLMs by connecting them to external knowledge sources, enabling them to access information beyond their training data.</p> <h3 id="how-rag-works">How RAG Works</h3> <div class="technique-diagram"> ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │ User Query │───▶│ Knowledge Base │───▶│ Retrieved Info │ │ │ │ Retrieval │ │ │ └─────────────────┘ └─────────────────┘ └─────────────────┘ │ │ ▼ ▼ ┌─────────────────┐ ┌─────────────────┐ │ Context │───▶│ Augmented │ │ Integration │ │ Generation │ └─────────────────┘ └─────────────────┘ </div> <p><strong>Knowledge Retrieval</strong>: When a user asks a question, RAG searches an external knowledge base for relevant information.</p> <p><strong>Context Integration</strong>: The retrieved information is provided to the LLM as additional context.</p> <p><strong>Augmented Generation</strong>: The LLM uses this additional context alongside its internal knowledge to generate a response.</p> <h3 id="why-rag-matters">Why RAG Matters</h3> <div class="comparison-highlight"> <strong>Addressing Temporal and Domain Boundaries:</strong> RAG directly addresses the temporal and domain boundary limitations by connecting LLMs to up-to-date information sources. </div> <p>RAG enables models to:</p> <ul> <li>Provide answers based on current information beyond their training cutoff</li> <li>Access specialized knowledge in domains where the model lacks depth</li> <li>Cite specific sources, increasing response reliability and transparency</li> </ul> <h2 id="finetuning">Finetuning</h2> <p>Finetuning adapts pre-trained LLMs to specific domains, tasks, or styles by additional training on specialized datasets.</p> <h3 id="how-finetuning-works">How Finetuning Works</h3> <div class="technique-box"> <strong>Process Overview:</strong> Transform general-purpose models into domain specialists through targeted training </div> <p><strong>Starting Point</strong>: Begin with a pre-trained LLM that has general knowledge.</p> <p><strong>Additional Training</strong>: Continue training the model on carefully selected datasets relevant to the target domain or task.</p> <p><strong>Parameter Adjustment</strong>: The model’s parameters are adjusted to optimize performance for the specific application.</p> <h3 id="why-finetuning-matters">Why Finetuning Matters</h3> <p>Finetuning addresses the domain boundary challenges by:</p> <ul> <li>Deepening the model’s expertise in specific knowledge areas</li> <li>Teaching the model to follow particular formats, styles, or processes</li> <li>Aligning the model’s outputs with specific organizational requirements or values</li> <li>Improving performance on specialized tasks like medical diagnosis or legal analysis</li> </ul> <h2 id="prompt-engineering">Prompt Engineering</h2> <p>Prompt engineering is the art and science of crafting effective instructions to guide LLM behavior and outputs.</p> <h3 id="how-prompt-engineering-works">How Prompt Engineering Works</h3> <div class="technique-box"> <strong>Approach:</strong> Strategic instruction design to optimize model performance without modification </div> <p><strong>Instruction Design</strong>: Carefully crafting the wording, structure, and guidance given to the LLM</p> <p><strong>Context Framing</strong>: Providing relevant background information and setting the stage for the response</p> <p><strong>Response Shaping</strong>: Using techniques like few-shot examples or specific formatting requirements</p> <h3 id="why-prompt-engineering-matters">Why Prompt Engineering Matters</h3> <p>Prompt engineering addresses contextual boundaries by:</p> <ul> <li>Helping models understand the specific requirements of a task</li> <li>Guiding models to produce outputs in desired formats or styles</li> <li>Encouraging more thorough reasoning or specific analytical approaches</li> <li>Improving response consistency and reliability without changing the model itself</li> </ul> <h2 id="similarities-between-the-approaches">Similarities Between the Approaches</h2> <p>All three techniques share important commonalities:</p> <p><strong>Knowledge Enhancement</strong>: Each approach helps LLMs overcome inherent knowledge limitations, though through different mechanisms.</p> <p><strong>Performance Optimization</strong>: All three aim to improve the quality, relevance, and reliability of LLM outputs.</p> <p><strong>Specialization</strong>: Each technique allows for adapting general-purpose LLMs to more specialized applications.</p> <p><strong>Boundary Management</strong>: All address the challenge of knowledge boundaries described in contemporary LLM research.</p> <h2 id="key-differences">Key Differences</h2> <p>Despite their similarities, these approaches differ significantly:</p> <p><strong>Implementation Complexity</strong>: Prompt engineering requires minimal technical infrastructure, while RAG needs retrieval systems and finetuning requires substantial computational resources.</p> <p><strong>Model Modification</strong>: Finetuning changes the model’s parameters, RAG adds external components, and prompt engineering works with the model as-is.</p> <p><strong>Adaptability</strong>: Prompt engineering offers the highest flexibility for quick adjustments, RAG allows dynamic knowledge updates, and finetuning provides deep but less flexible specialization.</p> <p><strong>Knowledge Recency</strong>: RAG provides the most current information access, prompt engineering can incorporate recent context, and finetuning is limited to training data vintage.</p> <h2 id="choosing-the-right-approach">Choosing the Right Approach</h2> <p>The optimal approach depends on specific requirements:</p> <div class="comparison-highlight"> <strong>Decision Framework:</strong> Select techniques based on your specific needs, resources, and constraints </div> <p><strong>Use RAG when</strong>: You need access to current information, specialized documents, or want to ensure factual accuracy with citations.</p> <p><strong>Use finetuning when</strong>: You need deep specialization in a particular domain, consistent adherence to specific patterns, or improved performance on specialized tasks.</p> <p><strong>Use prompt engineering when</strong>: You need flexibility, have limited technical resources, or want to quickly adapt how the model responds without changing its underlying capabilities.</p> <p><strong>Use combinations when</strong>: Most real-world applications benefit from combined approaches, such as using prompt engineering with a finetuned model connected to a RAG system.</p> <h2 id="conclusion">Conclusion</h2> <p>RAG, finetuning, and prompt engineering represent complementary approaches to extending LLM capabilities and addressing their inherent knowledge boundaries. While each approach has its strengths and limitations, they all contribute to making LLMs more useful, reliable, and applicable to real-world problems.</p> <div class="key-insight"> <strong>Future Perspective:</strong> As these technologies continue to evolve, we can expect even more sophisticated ways to enhance LLM performance and overcome their limitations through strategic combination of these techniques. </div> <p>Understanding these techniques is essential for organizations looking to deploy LLMs effectively. By selecting the right approach—or combination of approaches—for specific use cases, organizations can maximize the value of these powerful AI tools while managing their limitations appropriately.</p> <hr> <p><em>How has your experience been with these LLM enhancement techniques? Which approach has proven most effective for your specific use cases? Share your insights in the comments below.</em></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-03-26-rag-finetuning-prompt-engineering.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'danial-amin/danial-amin.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Danial Amin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Contet by <a href="https://danial-amin.github.io/" target="_blank">Danial Amin</a>. Last updated: July 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>